#  Working group at the IDEAL special semester on the foundations of deep learning

This is the webpage for the working group lead by Edgar Dobriban at the IDEAL [special semester on the foundations of deep learning](https://www.ideal.northwestern.edu/special-quarters/fall-2020/).

## Tentative topic: Robustness 

Mainly focusing on the theory of adversarial robustness.

Illustration of adversarial examples, from [https://gradientscience.org/intro_adversarial/](https://gradientscience.org/intro_adversarial/), by Aleksander MÄ…dry & Ludwig Schmidt:

[![Illustration of adversarial examples](https://gradientscience.org/images/piggie.png)](https://gradientscience.org/images/piggie.png "Illustration of adversarial examples") 

## Notes: 

[What causes adversarial examples? An overview of theoretical explanations](https://github.com/dobriban/ideal-working-group/blob/master/adv_note.pdf), by Edgar Dobriban (work in progress)

## Papers:

### Theoretical and semi-theoretical studies

[Adversarial Spheres](https://arxiv.org/abs/1801.02774)

[Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate](https://arxiv.org/abs/1806.05161)

[Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152)

[Adversarial Risk Bounds via Function Transformation](https://arxiv.org/abs/1810.09519)

[Theoretically Principled Trade-off between Robustness and Accuracy](https://arxiv.org/abs/1901.08573)

[Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)

[VC Classes are Adversarially Robustly Learnable, but Only Improperly](https://arxiv.org/abs/1902.04217)

[Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)

[Adversarial Training Can Hurt Generalization](https://arxiv.org/abs/1906.06032)

[Precise Tradeoffs in Adversarial Training for Linear Regression](https://arxiv.org/abs/2002.10477)

[Provable tradeoffs in adversarially robust classification](https://arxiv.org/abs/2006.05161)

### General reading
[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)

[Motivating the Rules of the Game for Adversarial Example Research](https://arxiv.org/abs/1807.06732)

[Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)

[On Evaluating Adversarial Robustness](https://arxiv.org/abs/1902.06705)


## Other materials: 

See section 6.1 of [my lecture notes](https://github.com/dobriban/Topics-in-deep-learning/blob/master/Lecture%20Notes/stat_991.pdf) for a collection of materials.

Good tutorial: [J. Z. Kolter and A. Madry: Adversarial Robustness - Theory and Practice (NeurIPS 2018 Tutorial)](www.youtube.com/watch?v=TwP-gKBQyic). [Website](adversarial-ml-tutorial.org/)

[CS 598: Special Topics on Adversarial Machine Learning](https://aisecure.github.io/TEACHING/2020_fall.html)

[Paper list](https://github.com/P2333/Papers-of-Robust-ML)

[robust-ml.org/](https://www.robust-ml.org/)


